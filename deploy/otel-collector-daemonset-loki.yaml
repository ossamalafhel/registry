---
# OpenTelemetry Collector DaemonSet with Loki Integration
# Implements AC2: Log Collection and Shipping with 10-second batching to Loki
---
# Namespace for OpenTelemetry Collector
apiVersion: v1
kind: Namespace
metadata:
  name: otel-system
  labels:
    environment: production
    component: logging

---
# ServiceAccount for OTel Collector
apiVersion: v1
kind: ServiceAccount
metadata:
  name: otel-collector
  namespace: otel-system
  labels:
    app: otel-collector
    component: logging

---
# ClusterRole for OTel Collector with enhanced permissions
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: otel-collector
  labels:
    app: otel-collector
    component: logging
rules:
  # Core resources
  - apiGroups: [""]
    resources:
      - pods
      - namespaces
      - nodes
      - endpoints
      - services
      - configmaps
      - secrets
      - events
    verbs:
      - get
      - watch
      - list
  # Pod logs access
  - apiGroups: [""]
    resources:
      - pods/log
    verbs:
      - get
      - list
  # Apps resources
  - apiGroups: ["apps"]
    resources:
      - deployments
      - daemonsets
      - statefulsets
      - replicasets
    verbs:
      - get
      - watch
      - list
  # Batch resources
  - apiGroups: ["batch"]
    resources:
      - jobs
      - cronjobs
    verbs:
      - get
      - watch
      - list
  # Events API access for k8s_events receiver
  - apiGroups: ["events.k8s.io"]
    resources:
      - events
    verbs:
      - get
      - watch
      - list

---
# ClusterRoleBinding for OTel Collector
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: otel-collector
  labels:
    app: otel-collector
    component: logging
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: otel-collector
subjects:
  - kind: ServiceAccount
    name: otel-collector
    namespace: otel-system

---
# ConfigMap for OTel Collector configuration with Loki
apiVersion: v1
kind: ConfigMap
metadata:
  name: otel-collector-config
  namespace: otel-system
  labels:
    app: otel-collector
    component: logging
data:
  otel-collector-config.yaml: |
    receivers:
      # Filelog receiver for container logs
      filelog:
        include:
          - /var/log/containers/*.log
        exclude:
          # Exclude OTel Collector's own logs to prevent infinite loop
          - /var/log/containers/*otel-collector*.log
        include_file_path: true
        include_file_name: false
        max_concurrent_files: 1024
        start_at: beginning
        operators:
          # Parse container log format (Docker/CRI-O JSON logs)
          - type: json_parser
            id: parser-docker
            timestamp:
              parse_from: attributes.time
              layout: '%Y-%m-%dT%H:%M:%S.%LZ'
            output: extract_metadata_from_filepath
          
          # Extract metadata from file path
          - type: regex_parser
            id: extract_metadata_from_filepath
            regex: '^/var/log/containers/(?P<pod_name>[^_]+)_(?P<namespace>[^_]+)_(?P<container_name>.*)-(?P<container_id>[^.]+)\.log$'
            parse_from: attributes["log.file.path"]
            output: move_attributes
          
          # Move parsed fields to resource attributes
          - type: move
            id: move_attributes
            from: attributes.pod_name
            to: resource["k8s.pod.name"]
          - type: move
            from: attributes.namespace
            to: resource["k8s.namespace.name"]
          - type: move
            from: attributes.container_name
            to: resource["k8s.container.name"]
          - type: move
            from: attributes.container_id
            to: resource["container.id"]
          - type: move
            from: attributes.log
            to: body
          - type: move
            from: attributes.stream
            to: attributes["log.iostream"]
          
          # Parse severity from stream (stdout/stderr)
          - type: add
            field: severity_text
            value: INFO
            if: 'attributes["log.iostream"] == "stdout"'
          - type: add
            field: severity_text
            value: ERROR
            if: 'attributes["log.iostream"] == "stderr"'

      # Kubernetes events receiver
      k8s_events:
        namespaces: [default, otel-system, mcp-registry]
        auth_type: serviceAccount

    processors:
      # Add Kubernetes metadata
      k8sattributes:
        auth_type: "serviceAccount"
        passthrough: false
        extract:
          metadata:
            - k8s.namespace.name
            - k8s.pod.name
            - k8s.pod.uid
            - k8s.deployment.name
            - k8s.node.name
            - k8s.container.name
          labels:
            - tag_name: app
              key: app
              from: pod
            - tag_name: team
              key: team
              from: pod
            - tag_name: component
              key: component
              from: pod
        pod_association:
          - sources:
            - from: resource_attribute
              name: k8s.pod.name
            - from: resource_attribute
              name: k8s.namespace.name

      # Resource processor for adding environment info
      resource:
        attributes:
          - key: environment
            value: production
            action: upsert
          - key: cluster.name
            value: mcp-registry-production
            action: upsert
          - key: service.name
            from_attribute: k8s.container.name
            action: insert

      # Transform processor for log formatting
      transform:
        error_mode: ignore
        log_statements:
          - context: log
            statements:
              # Add log level based on severity
              - set(attributes["level"], "INFO") where severity_text == "INFO"
              - set(attributes["level"], "ERROR") where severity_text == "ERROR"
              - set(attributes["level"], "WARN") where severity_text == "WARN"
              - set(attributes["level"], "DEBUG") where severity_text == "DEBUG"

      # Batch processor with 10-second timeout (AC2 requirement)
      batch/logs:
        timeout: 10s                    # AC2: Ship logs within 10 seconds
        send_batch_size: 8192           # Optimal batch size for Loki
        send_batch_max_size: 16384      # Maximum batch size

      # Memory limiter to prevent OOM
      memory_limiter:
        check_interval: 1s
        limit_percentage: 75
        spike_limit_percentage: 15

      # Filter processor to remove health check logs
      filter/logs:
        error_mode: ignore
        logs:
          exclude:
            match_type: regexp
            bodies:
              - ".*health.*check.*"
              - ".*readiness.*probe.*"
              - ".*liveness.*probe.*"

    exporters:
      # Loki exporter for log shipping
      loki:
        endpoint: "http://loki:3100/loki/api/v1/push"  # Update with actual Loki endpoint
        headers:
          X-Scope-OrgID: "mcp-registry"
        tls:
          insecure: true  # Set to false in production with proper TLS
        retry_on_failure:
          enabled: true
          initial_interval: 1s
          max_interval: 30s
          max_elapsed_time: 300s
        timeout: 30s
        format: json
        labels:
          attributes:
            k8s.namespace.name: "namespace"
            k8s.pod.name: "pod"
            k8s.container.name: "container"
            k8s.node.name: "node"
            k8s.deployment.name: "deployment"
            environment: "environment"
            cluster.name: "cluster"
            app: "app"
            team: "team"
            component: "component"
            level: "level"
          resource:
            service.name: "service"

      # Debug exporter for troubleshooting
      debug:
        verbosity: basic
        sampling_initial: 5
        sampling_thereafter: 100

    extensions:
      # Health check extension
      health_check:
        endpoint: 0.0.0.0:13133
        path: "/health"
        check_collector_pipeline:
          enabled: true
          interval: 5s
          exporter_failure_threshold: 5

      # Performance profiler
      pprof:
        endpoint: 0.0.0.0:1777

      # Memory ballast for heap optimization
      memory_ballast:
        size_in_percentage: 20

    service:
      extensions: [health_check, pprof, memory_ballast]
      
      pipelines:
        # Main logs pipeline with Loki export (AC2 implementation)
        logs:
          receivers: [filelog]
          processors: 
            - memory_limiter
            - k8sattributes
            - resource
            - transform
            - filter/logs
            - batch/logs           # 10-second batching as per AC2
          exporters: [loki, debug]
        
        # Kubernetes events pipeline
        logs/k8s_events:
          receivers: [k8s_events]
          processors:
            - memory_limiter
            - resource
            - batch/logs
          exporters: [loki]

      telemetry:
        logs:
          level: info
          initial_fields:
            service.name: otel-collector
            service.version: 0.1.0
        metrics:
          level: detailed
          address: 0.0.0.0:8889

---
# DaemonSet for OTel Collector with Loki integration
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: otel-collector
  namespace: otel-system
  labels:
    app: otel-collector
    component: logging
spec:
  selector:
    matchLabels:
      app: otel-collector
  updateStrategy:
    type: RollingUpdate
    rollingUpdate:
      maxUnavailable: 1
  template:
    metadata:
      labels:
        app: otel-collector
        component: logging
      annotations:
        prometheus.io/scrape: "true"
        prometheus.io/port: "8889"
        prometheus.io/path: "/metrics"
        checksum/config: "{{ .Values.configChecksum }}"  # Force pod restart on config change
    spec:
      serviceAccountName: otel-collector
      hostNetwork: false
      dnsPolicy: ClusterFirst
      containers:
        - name: otel-collector
          image: otel/opentelemetry-collector-contrib:0.91.0
          imagePullPolicy: IfNotPresent
          args:
            - --config=/conf/otel-collector-config.yaml
          env:
            # Kubernetes metadata
            - name: K8S_NODE_NAME
              valueFrom:
                fieldRef:
                  fieldPath: spec.nodeName
            - name: K8S_POD_NAME
              valueFrom:
                fieldRef:
                  fieldPath: metadata.name
            - name: K8S_POD_NAMESPACE
              valueFrom:
                fieldRef:
                  fieldPath: metadata.namespace
            - name: K8S_POD_IP
              valueFrom:
                fieldRef:
                  fieldPath: status.podIP
            - name: K8S_POD_UID
              valueFrom:
                fieldRef:
                  fieldPath: metadata.uid
            # Memory optimization
            - name: GOMEMLIMIT
              value: "400MiB"
            # Loki configuration (can be overridden)
            - name: LOKI_ENDPOINT
              value: "http://loki:3100/loki/api/v1/push"
            - name: LOKI_TENANT_ID
              value: "mcp-registry"
            - name: LOKI_TLS_INSECURE
              value: "true"
            # Environment configuration
            - name: ENVIRONMENT
              value: "production"
            - name: CLUSTER_NAME
              value: "mcp-registry-production"
          ports:
            - name: metrics
              containerPort: 8889
              protocol: TCP
            - name: healthcheck
              containerPort: 13133
              protocol: TCP
            - name: pprof
              containerPort: 1777
              protocol: TCP
          livenessProbe:
            httpGet:
              path: /health
              port: 13133
            initialDelaySeconds: 30
            periodSeconds: 10
            timeoutSeconds: 5
            failureThreshold: 3
          readinessProbe:
            httpGet:
              path: /health
              port: 13133
            initialDelaySeconds: 10
            periodSeconds: 10
            timeoutSeconds: 5
            failureThreshold: 3
          resources:
            requests:
              cpu: 100m
              memory: 256Mi
            limits:
              cpu: 500m
              memory: 512Mi
          volumeMounts:
            - name: otel-collector-config
              mountPath: /conf
              readOnly: true
            - name: varlog
              mountPath: /var/log
              readOnly: true
            - name: varlibdockercontainers
              mountPath: /var/lib/docker/containers
              readOnly: true
            - name: otel-storage
              mountPath: /var/lib/otel
          securityContext:
            capabilities:
              drop:
                - ALL
            readOnlyRootFilesystem: false  # Need write access for checkpoint storage
            allowPrivilegeEscalation: false
            runAsNonRoot: true
            runAsUser: 65534  # nobody user
      volumes:
        - name: otel-collector-config
          configMap:
            name: otel-collector-config
            items:
              - key: otel-collector-config.yaml
                path: otel-collector-config.yaml
        - name: varlog
          hostPath:
            path: /var/log
            type: Directory
        - name: varlibdockercontainers
          hostPath:
            path: /var/lib/docker/containers
            type: Directory
        - name: otel-storage
          emptyDir: {}
      tolerations:
        - key: node-role.kubernetes.io/master
          operator: Exists
          effect: NoSchedule
        - key: node-role.kubernetes.io/control-plane
          operator: Exists
          effect: NoSchedule

---
# Service for OTel Collector metrics endpoint
apiVersion: v1
kind: Service
metadata:
  name: otel-collector-metrics
  namespace: otel-system
  labels:
    app: otel-collector
    component: logging
spec:
  selector:
    app: otel-collector
  type: ClusterIP
  ports:
    - name: metrics
      port: 8889
      targetPort: 8889
      protocol: TCP
    - name: healthcheck
      port: 13133
      targetPort: 13133
      protocol: TCP

---
# ServiceMonitor for Prometheus scraping (if using Prometheus Operator)
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: otel-collector
  namespace: otel-system
  labels:
    app: otel-collector
    component: logging
spec:
  selector:
    matchLabels:
      app: otel-collector
  endpoints:
    - port: metrics
      interval: 30s
      path: /metrics